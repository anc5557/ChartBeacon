"""
ë°ì´í„° ì±„ìš°ê¸° ëª¨ë“ˆ
Yahoo Financeì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ìº”ë“¤, ì§€í‘œ, ìš”ì•½ ë°ì´í„°ë¥¼ ê³„ì‚°í•˜ê³  ì €ì¥
"""

import yfinance as yf
import pandas as pd
import pandas_ta as ta
import asyncpg
from datetime import datetime, timezone
from typing import List, Optional
import logging
import os

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì • (asyncpg ì§ì ‘ ì—°ê²°ìš©)
# í™˜ê²½ì— ë”°ë¥¸ DATABASE_URL ì„¤ì •
ENVIRONMENT = os.getenv("ENVIRONMENT", "development")
if ENVIRONMENT == "production" or os.getenv("DOCKER_ENV"):
    # ë„ì»¤ í™˜ê²½ì—ì„œëŠ” postgres í˜¸ìŠ¤íŠ¸ëª… ì‚¬ìš©
    DATABASE_URL = os.getenv(
        "DATABASE_URL", "postgresql+asyncpg://chartbeacon:chartbeacon123@postgres:5432/chartbeacon"
    )
else:
    # ë¡œì»¬ ê°œë°œí™˜ê²½ì—ì„œëŠ” localhost ì‚¬ìš©
    DATABASE_URL = os.getenv(
        "DATABASE_URL", "postgresql+asyncpg://chartbeacon:chartbeacon123@localhost:5432/chartbeacon"
    )
# asyncpg ì§ì ‘ ì—°ê²°ì„ ìœ„í•´ prefix ì œê±°
if DATABASE_URL.startswith("postgresql+asyncpg://"):
    DATABASE_URL = DATABASE_URL.replace("postgresql+asyncpg://", "postgresql://")


async def fill_historical_data(ticker: str, timeframes: List[str] = None, period: str = "max"):
    """
    íŠ¹ì • ì¢…ëª©ì˜ ê³¼ê±° ë°ì´í„°ë¥¼ ì±„ìš°ëŠ” ë©”ì¸ í•¨ìˆ˜

    Args:
        ticker: ì¢…ëª© ì½”ë“œ
        timeframes: íƒ€ì„í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ ['5m', '1h', '1d', '5d', '1mo', '3mo']
        period: ë°ì´í„° ê¸°ê°„ (max = ìµœëŒ€í•œ ê¸´ ê¸°ê°„ìœ¼ë¡œ MA200 ë“± ì¶©ë¶„í•œ ë°ì´í„° í™•ë³´)
    """
    if timeframes is None:
        timeframes = ["5m", "1h", "1d", "5d", "1mo", "3mo"]

    logger.info(f"ğŸš€ Starting data fill for {ticker}, timeframes: {timeframes}, period: {period}")

    try:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        conn = await asyncpg.connect(DATABASE_URL)

        # ì‹¬ë³¼ ID ì¡°íšŒ
        symbol_id = await get_symbol_id(conn, ticker)
        if not symbol_id:
            logger.error(f"Symbol {ticker} not found in database")
            return

        for timeframe in timeframes:
            logger.info(f"Processing {ticker} - {timeframe}")

            # 1. Yahoo Financeì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            df = fetch_yahoo_data(ticker, timeframe, period)
            if df is None or df.empty:
                logger.warning(f"No data found for {ticker} - {timeframe}")
                continue

            # 2. ìº”ë“¤ ë°ì´í„° ì €ì¥
            await save_candle_data(conn, symbol_id, timeframe, df)

            # 3. ì§€í‘œ ê³„ì‚° ë° ì €ì¥
            await calculate_and_save_indicators(conn, symbol_id, timeframe, df)

            # 4. ì´ë™í‰ê·  ê³„ì‚° ë° ì €ì¥
            await calculate_and_save_moving_averages(conn, symbol_id, timeframe, df)

            # 5. ìš”ì•½ ê³„ì‚° ë° ì €ì¥
            await calculate_and_save_summary(conn, symbol_id, timeframe, df)

            logger.info(f"Completed processing {ticker} - {timeframe}")

        await conn.close()
        logger.info(f"âœ… Data fill completed for {ticker}")

    except Exception as e:
        logger.error(f"âŒ Error filling data for {ticker}: {str(e)}")
        raise


def fetch_yahoo_data(ticker: str, timeframe: str, period: str = "max") -> Optional[pd.DataFrame]:
    """
    Yahoo Financeì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    ì‹œê°„ëŒ€ ì²˜ë¦¬: ê±°ë˜ì†Œë³„ í˜„ì§€ ì‹œê°„ìœ¼ë¡œ ë°›ì•„ì„œ UTCë¡œ ë³€í™˜í•˜ì—¬ DBì— ì €ì¥
    """
    try:
        # íƒ€ì„í”„ë ˆì„ë³„ interval ë§¤í•‘
        interval_map = {
            "5m": "5m",
            "1h": "1h",
            "1d": "1d",
            "5d": "5d",
            "1mo": "1mo",
            "3mo": "3mo",
        }

        interval = interval_map.get(timeframe)
        if not interval:
            logger.error(f"Unsupported timeframe: {timeframe}")
            return None

        # ë‹¨ê¸° ë°ì´í„°ëŠ” ê¸°ê°„ ì œí•œì´ ìˆìŒ, ì¥ê¸° ë°ì´í„°ëŠ” ì¶©ë¶„í•œ ê¸°ê°„ í™•ë³´
        if timeframe == "5m":
            period = "60d"  # 5ë¶„: ìµœëŒ€ 60ì¼
        elif timeframe == "1h":
            period = "730d"  # 1ì‹œê°„: ìµœëŒ€ 730ì¼
        elif timeframe in ["5d", "1mo", "3mo"]:
            # ì¥ê¸° ë°ì´í„°ëŠ” ë” ê¸´ ê¸°ê°„ ì‚¬ìš©í•˜ì—¬ MA200ê¹Œì§€ ì¶©ë¶„íˆ í™•ë³´
            if timeframe == "5d":
                period = "10y"  # 5ì¼ë´‰: 10ë…„ (MA200 í™•ë³´)
            elif timeframe == "1mo":
                period = "max"  # 1ì›”ë´‰: ìµœëŒ€ ê¸°ê°„ (MA200 í™•ë³´)
            elif timeframe == "3mo":
                period = "max"  # 3ì›”ë´‰: ìµœëŒ€ ê¸°ê°„ (MA50, MA200 í™•ë³´)
        elif timeframe == "1d":
            # 1ì¼ë´‰ë„ ì¶©ë¶„í•œ ê¸°ê°„ í™•ë³´
            if period == "max":
                period = "max"
            else:
                period = "5y"  # ê¸°ë³¸ì ìœ¼ë¡œ 5ë…„

        logger.info(f"Fetching {ticker} data: interval={interval}, period={period}")

        stock = yf.Ticker(ticker)
        df = stock.history(period=period, interval=interval)

        if df.empty:
            logger.warning(f"No data returned for {ticker}")
            return None

        # ì‹œê°„ëŒ€ ì²˜ë¦¬: Yahoo FinanceëŠ” ê±°ë˜ì†Œë³„ í˜„ì§€ ì‹œê°„ìœ¼ë¡œ ì œê³µ
        # ëª¨ë“  ë°ì´í„°ë¥¼ UTCë¡œ í†µì¼í•˜ì—¬ DBì— ì €ì¥
        logger.info(f"Processing timezone for {ticker} - Original timezone: {df.index.tz}")
        logger.info(f"Index sample: {df.index[:3].tolist() if len(df.index) > 0 else 'Empty'}")

        try:
            # Yahoo Finance ë°ì´í„°ì˜ ì‹œê°„ëŒ€ ì²˜ë¦¬
            if df.index.tz is not None:
                # ì´ë¯¸ timezone ì •ë³´ê°€ ìˆìœ¼ë©´ UTCë¡œ ë³€í™˜
                logger.info(f"Converting from {df.index.tz} to UTC")
                df.index = df.index.tz_convert("UTC")
            else:
                # timezone ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ê±°ë˜ì†Œë³„ë¡œ ì¶”ì •
                if ticker.endswith(".KS"):
                    # í•œêµ­ ì£¼ì‹: KST(Asia/Seoul)ë¡œ ê°€ì •í•˜ê³  UTCë¡œ ë³€í™˜
                    logger.info("Korean stock: Localizing to Asia/Seoul then converting to UTC")
                    df.index = df.index.tz_localize("Asia/Seoul").tz_convert("UTC")
                else:
                    # ê¸°íƒ€ ì£¼ì‹: NYSE/NASDAQ ë“±ì€ ëŒ€ë¶€ë¶„ EST/EDTë¡œ ê°€ì •
                    # í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” Yahoo Financeê°€ ì´ë¯¸ ì ì ˆí•œ ì‹œê°„ëŒ€ë¡œ ì œê³µí•˜ë¯€ë¡œ UTCë¡œ ê°€ì •
                    logger.info("Non-Korean stock: Localizing to UTC")
                    df.index = df.index.tz_localize("UTC")

            logger.info(f"After timezone processing - Index timezone: {df.index.tz}")

        except Exception as tz_error:
            logger.error(f"Timezone conversion error for {ticker}: {tz_error}")
            logger.error(f"Error type: {type(tz_error)}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")

            # ì—ëŸ¬ ë°œìƒ ì‹œ fallback: timezone ì •ë³´ ì œê±°í•˜ê³  UTCë¡œ ì„¤ì •
            try:
                if hasattr(df.index, "tz_localize"):
                    df.index = df.index.tz_localize(None).tz_localize("UTC")
                    logger.info("Fallback: Removed timezone and re-localized to UTC")
                else:
                    df.index = pd.to_datetime(df.index, utc=True)
                    logger.info("Fallback: Converted to UTC using pd.to_datetime")
            except Exception as fallback_error:
                logger.error(f"Fallback also failed: {fallback_error}")
                logger.warning("Proceeding without timezone conversion")

        df.columns = df.columns.str.lower()
        df = df.reset_index()

        # ë””ë²„ê¹…: ì‹¤ì œ ì»¬ëŸ¼ëª… í™•ì¸
        logger.info(f"DataFrame columns after reset_index: {list(df.columns)}")

        # ì¸ë±ìŠ¤ ì»¬ëŸ¼ëª…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í™•ì¸ í›„ ë³€ê²½
        if "date" in df.columns:
            df.rename(columns={"date": "ts"}, inplace=True)
        elif "datetime" in df.columns:
            df.rename(columns={"datetime": "ts"}, inplace=True)
        elif "Datetime" in df.columns:
            df.rename(columns={"Datetime": "ts"}, inplace=True)
        else:
            # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ì‹œê°„ ì»¬ëŸ¼ì¼ ê°€ëŠ¥ì„±
            first_col = df.columns[0]
            logger.warning(
                f"Expected 'date' or 'datetime' column not found. Using first column: {first_col}"
            )
            df.rename(columns={first_col: "ts"}, inplace=True)

        # í•œêµ­ ì£¼ì‹ì˜ ê²½ìš° ì •ê·œì¥ ì‹œê°„ë§Œ í•„í„°ë§ (5ë¶„, 1ì‹œê°„ë´‰ë§Œ)
        if ticker.endswith(".KS") and timeframe in ["5m", "1h"]:
            logger.info(f"Filtering regular trading hours for Korean stock {ticker}")

            # ts ì»¬ëŸ¼ì´ ì´ë¯¸ UTC ì‹œê°„ì´ë¯€ë¡œ KSTë¡œ ë³€í™˜í•´ì„œ í•„í„°ë§
            ts_series = pd.to_datetime(df["ts"])
            
            # UTCì—ì„œ KST(+9ì‹œê°„)ë¡œ ë³€í™˜
            if ts_series.dt.tz is not None:
                df_ts_kst = ts_series.dt.tz_convert("Asia/Seoul")
            else:
                df_ts_kst = ts_series.dt.tz_localize("UTC").dt.tz_convert("Asia/Seoul")

            df["hour"] = df_ts_kst.dt.hour
            df["minute"] = df_ts_kst.dt.minute

            # í•œêµ­ ì •ê·œì¥ ì‹œê°„: 09:00 ~ 15:30 (15:30 í¬í•¨)
            regular_hours = (
                ((df["hour"] == 9) & (df["minute"] >= 0))
                | ((df["hour"] >= 10) & (df["hour"] <= 14))
                | ((df["hour"] == 15) & (df["minute"] <= 30))
            )

            original_count = len(df)
            df = df[regular_hours].drop(["hour", "minute"], axis=1)
            filtered_count = len(df)

            logger.info(f"Regular hours filtering: {original_count} -> {filtered_count} records")

            if filtered_count == 0:
                logger.warning("All data filtered out! This might indicate timezone issues.")

        logger.info(f"Final DataFrame columns: {list(df.columns)}")
        logger.info(f"Fetched {len(df)} records for {ticker} - {timeframe}")
        return df

    except Exception as e:
        logger.error(f"Error fetching data for {ticker}: {str(e)}")
        return None


async def get_symbol_id(conn: asyncpg.Connection, ticker: str) -> Optional[int]:
    """
    ì‹¬ë³¼ ID ì¡°íšŒ
    """
    result = await conn.fetchrow("SELECT id FROM symbols WHERE ticker = $1", ticker)
    return result["id"] if result else None


async def save_candle_data(
    conn: asyncpg.Connection, symbol_id: int, timeframe: str, df: pd.DataFrame
):
    """
    ìº”ë“¤ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (UPSERT)
    """
    logger.info(f"Saving {len(df)} candle records for symbol_id={symbol_id}, timeframe={timeframe}")

    # ë°°ì¹˜ insertë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
    records = []
    for _, row in df.iterrows():
        records.append(
            (
                symbol_id,
                timeframe,
                row["ts"],
                float(row["open"]),
                float(row["high"]),
                float(row["low"]),
                float(row["close"]),
                float(row["volume"]),
                datetime.now(timezone.utc),  # ingested_atì€ í˜„ì¬ UTC ì‹œê°„
            )
        )

    # UPSERT ì¿¼ë¦¬
    query = """
        INSERT INTO candles_raw (symbol_id, timeframe, ts, open, high, low, close, volume, ingested_at)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
        ON CONFLICT (symbol_id, timeframe, ts) 
        DO UPDATE SET 
            open = EXCLUDED.open,
            high = EXCLUDED.high,
            low = EXCLUDED.low,
            close = EXCLUDED.close,
            volume = EXCLUDED.volume,
            ingested_at = EXCLUDED.ingested_at
    """

    await conn.executemany(query, records)
    logger.info(f"Saved {len(records)} candle records")


async def calculate_and_save_indicators(
    conn: asyncpg.Connection, symbol_id: int, timeframe: str, df: pd.DataFrame
):
    """
    ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ë° ì €ì¥
    """
    logger.info(f"Calculating indicators for symbol_id={symbol_id}, timeframe={timeframe}")

    # ë°ì´í„° ê¸¸ì´ í™•ì¸
    if len(df) < 30:
        logger.warning(f"Insufficient data for indicators calculation: {len(df)} rows")
        return

    # ì§€í‘œ ê³„ì‚° (ì•ˆì „í•œ ì²˜ë¦¬)
    try:
        # RSI
        df["rsi14"] = ta.rsi(df["close"], length=14)

        # Stochastic
        stoch = ta.stoch(df["high"], df["low"], df["close"], k=9, d=6)
        if stoch is not None and not stoch.empty:
            df["stoch_k"] = stoch.get("STOCHk_9_6_3", pd.Series(index=df.index, dtype=float))
            df["stoch_d"] = stoch.get("STOCHd_9_6_3", pd.Series(index=df.index, dtype=float))
        else:
            df["stoch_k"] = pd.Series(index=df.index, dtype=float)
            df["stoch_d"] = pd.Series(index=df.index, dtype=float)

        # MACD
        macd = ta.macd(df["close"], fast=12, slow=26, signal=9)
        if macd is not None and not macd.empty:
            df["macd"] = macd.get("MACD_12_26_9", pd.Series(index=df.index, dtype=float))
            df["macd_signal"] = macd.get("MACDs_12_26_9", pd.Series(index=df.index, dtype=float))
        else:
            df["macd"] = pd.Series(index=df.index, dtype=float)
            df["macd_signal"] = pd.Series(index=df.index, dtype=float)

        # ADX
        adx = ta.adx(df["high"], df["low"], df["close"], length=14)
        if adx is not None and not adx.empty:
            df["adx14"] = adx.get("ADX_14", pd.Series(index=df.index, dtype=float))
        else:
            df["adx14"] = pd.Series(index=df.index, dtype=float)

        # CCI
        df["cci14"] = ta.cci(df["high"], df["low"], df["close"], length=14)
        if df["cci14"] is None:
            df["cci14"] = pd.Series(index=df.index, dtype=float)

        # ATR
        df["atr14"] = ta.atr(df["high"], df["low"], df["close"], length=14)
        if df["atr14"] is None:
            df["atr14"] = pd.Series(index=df.index, dtype=float)

        # Williams %R
        df["willr14"] = ta.willr(df["high"], df["low"], df["close"], length=14)
        if df["willr14"] is None:
            df["willr14"] = pd.Series(index=df.index, dtype=float)

        # Ultimate Oscillator
        df["ultosc"] = ta.uo(df["high"], df["low"], df["close"])
        if df["ultosc"] is None:
            df["ultosc"] = pd.Series(index=df.index, dtype=float)

        # ROC
        df["roc"] = ta.roc(df["close"], length=12)
        if df["roc"] is None:
            df["roc"] = pd.Series(index=df.index, dtype=float)

        # Bull/Bear Power
        ema13 = ta.ema(df["close"], length=13)
        if ema13 is not None:
            df["bull_bear"] = df["high"] - ema13
        else:
            df["bull_bear"] = pd.Series(index=df.index, dtype=float)

        # ë†’ì€/ë‚®ì€ ì§€í‘œ (High - Low)
        df["highlow14"] = df["high"].rolling(14).max() - df["low"].rolling(14).min()

    except Exception as e:
        logger.error(f"Error calculating indicators: {str(e)}")
        # ì‹¤íŒ¨ ì‹œ ë¹ˆ Seriesë¡œ ì±„ìš°ê¸°
        for col in [
            "rsi14",
            "stoch_k",
            "stoch_d",
            "macd",
            "macd_signal",
            "adx14",
            "cci14",
            "atr14",
            "willr14",
            "ultosc",
            "roc",
            "bull_bear",
            "highlow14",
        ]:
            if col not in df.columns:
                df[col] = pd.Series(index=df.index, dtype=float)

    # ë°ì´í„° ì €ì¥
    records = []
    for _, row in df.iterrows():
        if pd.isna(row["rsi14"]):  # ì´ˆê¸° ëª‡ ê°œ í–‰ì€ ì§€í‘œê°€ ê³„ì‚°ë˜ì§€ ì•ŠìŒ
            continue

        records.append(
            (
                symbol_id,
                timeframe,
                row["ts"],
                safe_float(row.get("rsi14")),
                safe_float(row.get("stoch_k")),
                safe_float(row.get("stoch_d")),
                safe_float(row.get("macd")),
                safe_float(row.get("macd_signal")),
                safe_float(row.get("adx14")),
                safe_float(row.get("cci14")),
                safe_float(row.get("atr14")),
                safe_float(row.get("willr14")),
                safe_float(row.get("highlow14")),
                safe_float(row.get("ultosc")),
                safe_float(row.get("roc")),
                safe_float(row.get("bull_bear")),
                datetime.now(timezone.utc),  # calc_atì€ í˜„ì¬ UTC ì‹œê°„
            )
        )

    if records:
        query = """
            INSERT INTO indicators (
                symbol_id, timeframe, ts, rsi14, stoch_k, stoch_d, 
                macd, macd_signal, adx14, cci14, atr14, willr14, highlow14, 
                ultosc, roc, bull_bear, calc_at
            )
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17)
            ON CONFLICT (symbol_id, timeframe, ts) 
            DO UPDATE SET 
                rsi14 = EXCLUDED.rsi14,
                stoch_k = EXCLUDED.stoch_k,
                stoch_d = EXCLUDED.stoch_d,
                macd = EXCLUDED.macd,
                macd_signal = EXCLUDED.macd_signal,
                adx14 = EXCLUDED.adx14,
                cci14 = EXCLUDED.cci14,
                atr14 = EXCLUDED.atr14,
                willr14 = EXCLUDED.willr14,
                highlow14 = EXCLUDED.highlow14,
                ultosc = EXCLUDED.ultosc,
                roc = EXCLUDED.roc,
                bull_bear = EXCLUDED.bull_bear,
                calc_at = EXCLUDED.calc_at
        """

        await conn.executemany(query, records)
        logger.info(f"Saved {len(records)} indicator records")


async def calculate_and_save_moving_averages(
    conn: asyncpg.Connection, symbol_id: int, timeframe: str, df: pd.DataFrame
):
    """
    ì´ë™í‰ê·  ê³„ì‚° ë° ì €ì¥
    """
    logger.info(f"Calculating moving averages for symbol_id={symbol_id}, timeframe={timeframe}")

    # ì´ë™í‰ê·  ê³„ì‚°
    df["ma5"] = ta.sma(df["close"], length=5)
    df["ema5"] = ta.ema(df["close"], length=5)
    df["ma10"] = ta.sma(df["close"], length=10)
    df["ema10"] = ta.ema(df["close"], length=10)
    df["ma20"] = ta.sma(df["close"], length=20)
    df["ema20"] = ta.ema(df["close"], length=20)
    df["ma50"] = ta.sma(df["close"], length=50)
    df["ma100"] = ta.sma(df["close"], length=100)
    df["ma200"] = ta.sma(df["close"], length=200)

    # ë°ì´í„° ì €ì¥
    records = []
    for _, row in df.iterrows():
        if pd.isna(row["ma5"]):  # ì´ˆê¸° ëª‡ ê°œ í–‰ì€ ì´ë™í‰ê· ì´ ê³„ì‚°ë˜ì§€ ì•ŠìŒ
            continue

        records.append(
            (
                symbol_id,
                timeframe,
                row["ts"],
                safe_float(row.get("ma5")),
                safe_float(row.get("ema5")),
                safe_float(row.get("ma10")),
                safe_float(row.get("ema10")),
                safe_float(row.get("ma20")),
                safe_float(row.get("ema20")),
                safe_float(row.get("ma50")),
                safe_float(row.get("ma100")),
                safe_float(row.get("ma200")),
                datetime.now(timezone.utc),  # calc_atì€ í˜„ì¬ UTC ì‹œê°„
            )
        )

    if records:
        query = """
            INSERT INTO moving_avgs (
                symbol_id, timeframe, ts, ma5, ema5, ma10, ema10, 
                ma20, ema20, ma50, ma100, ma200, calc_at
            )
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
            ON CONFLICT (symbol_id, timeframe, ts) 
            DO UPDATE SET 
                ma5 = EXCLUDED.ma5,
                ema5 = EXCLUDED.ema5,
                ma10 = EXCLUDED.ma10,
                ema10 = EXCLUDED.ema10,
                ma20 = EXCLUDED.ma20,
                ema20 = EXCLUDED.ema20,
                ma50 = EXCLUDED.ma50,
                ma100 = EXCLUDED.ma100,
                ma200 = EXCLUDED.ma200,
                calc_at = EXCLUDED.calc_at
        """

        await conn.executemany(query, records)
        logger.info(f"Saved {len(records)} moving average records")


async def calculate_and_save_summary(
    conn: asyncpg.Connection, symbol_id: int, timeframe: str, df: pd.DataFrame
):
    """
    ìš”ì•½ ì ìˆ˜ ê³„ì‚° ë° ì €ì¥
    """
    logger.info(f"Calculating summary for symbol_id={symbol_id}, timeframe={timeframe}")

    # ìµœì‹  200ê°œ í–‰ë§Œ ì²˜ë¦¬ (ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” êµ¬ê°„)
    df_recent = df.tail(200).copy()

    records = []
    for _, row in df_recent.iterrows():
        # ì§€í‘œ ê¸°ë°˜ ì‹œê·¸ë„ ê³„ì‚°
        signals = calculate_signals(row)

        buy_cnt = sum(1 for s in signals if s == "BUY")
        sell_cnt = sum(1 for s in signals if s == "SELL")
        neutral_cnt = sum(1 for s in signals if s == "NEUTRAL")

        # ìµœì¢… ë ˆë²¨ ê²°ì •
        total_signals = buy_cnt + sell_cnt + neutral_cnt
        if total_signals == 0:
            level = "NEUTRAL"
        elif buy_cnt >= (total_signals * 2 // 3):
            level = "STRONG_BUY"
        elif buy_cnt > sell_cnt:
            level = "BUY"
        elif sell_cnt >= (total_signals * 2 // 3):
            level = "STRONG_SELL"
        elif sell_cnt > buy_cnt:
            level = "SELL"
        else:
            level = "NEUTRAL"

        records.append(
            (
                symbol_id,
                timeframe,
                row["ts"],
                buy_cnt,
                sell_cnt,
                neutral_cnt,
                level,
                datetime.now(timezone.utc),  # scored_atì€ í˜„ì¬ UTC ì‹œê°„
            )
        )

    if records:
        query = """
            INSERT INTO summary (
                symbol_id, timeframe, ts, buy_cnt, sell_cnt, 
                neutral_cnt, level, scored_at
            )
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
            ON CONFLICT (symbol_id, timeframe, ts) 
            DO UPDATE SET 
                buy_cnt = EXCLUDED.buy_cnt,
                sell_cnt = EXCLUDED.sell_cnt,
                neutral_cnt = EXCLUDED.neutral_cnt,
                level = EXCLUDED.level,
                scored_at = EXCLUDED.scored_at
        """

        await conn.executemany(query, records)
        logger.info(f"Saved {len(records)} summary records")


def calculate_signals(row) -> List[str]:
    """
    ê°œë³„ í–‰ì˜ ì§€í‘œ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œê·¸ë„ ê³„ì‚°
    """
    signals = []

    # RSI
    if pd.notna(row.get("rsi14")):
        if row["rsi14"] > 70:
            signals.append("SELL")
        elif row["rsi14"] < 30:
            signals.append("BUY")
        else:
            signals.append("NEUTRAL")

    # Stochastic
    if pd.notna(row.get("stoch_k")):
        if row["stoch_k"] > 80:
            signals.append("SELL")
        elif row["stoch_k"] < 20:
            signals.append("BUY")
        else:
            signals.append("NEUTRAL")

    # MACD
    if pd.notna(row.get("macd")) and pd.notna(row.get("macd_signal")):
        if row["macd"] > row["macd_signal"]:
            signals.append("BUY")
        elif row["macd"] < row["macd_signal"]:
            signals.append("SELL")
        else:
            signals.append("NEUTRAL")

    # CCI
    if pd.notna(row.get("cci14")):
        if row["cci14"] > 100:
            signals.append("BUY")
        elif row["cci14"] < -100:
            signals.append("SELL")
        else:
            signals.append("NEUTRAL")

    # Williams %R
    if pd.notna(row.get("willr14")):
        if row["willr14"] > -20:
            signals.append("SELL")
        elif row["willr14"] < -80:
            signals.append("BUY")
        else:
            signals.append("NEUTRAL")

    # ROC
    if pd.notna(row.get("roc")):
        if row["roc"] > 0:
            signals.append("BUY")
        elif row["roc"] < 0:
            signals.append("SELL")
        else:
            signals.append("NEUTRAL")

    # ì´ë™í‰ê·  ì‹œê·¸ë„ë“¤
    ma_signals = []
    for ma_col in ["ma5", "ema5", "ma10", "ema10", "ma20", "ema20", "ma50", "ma100", "ma200"]:
        if pd.notna(row.get(ma_col)) and pd.notna(row.get("close")):
            if row["close"] > row[ma_col]:
                ma_signals.append("BUY")
            elif row["close"] < row[ma_col]:
                ma_signals.append("SELL")
            else:
                ma_signals.append("NEUTRAL")

    signals.extend(ma_signals)
    return signals


def safe_float(value) -> Optional[float]:
    """
    ì•ˆì „í•œ float ë³€í™˜ (NaN ì²˜ë¦¬)
    """
    if pd.isna(value):
        return None
    try:
        return float(value)
    except (ValueError, TypeError):
        return None

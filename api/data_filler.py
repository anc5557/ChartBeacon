"""
Îç∞Ïù¥ÌÑ∞ Ï±ÑÏö∞Í∏∞ Î™®Îìà
Yahoo FinanceÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏ†∏ÏôÄÏÑú Ï∫îÎì§, ÏßÄÌëú, ÏöîÏïΩ Îç∞Ïù¥ÌÑ∞Î•º Í≥ÑÏÇ∞ÌïòÍ≥† Ï†ÄÏû•
"""

import yfinance as yf
import pandas as pd
import pandas_ta as ta
import asyncpg
from datetime import datetime, timezone
from typing import List, Optional
import logging
import os

# Î°úÍ±∞ ÏÑ§Ï†ï
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÏÑ§Ï†ï (asyncpg ÏßÅÏ†ë Ïó∞Í≤∞Ïö©)
# ÌôòÍ≤ΩÏóê Îî∞Î•∏ DATABASE_URL ÏÑ§Ï†ï
ENVIRONMENT = os.getenv("ENVIRONMENT", "development")
if ENVIRONMENT == "production" or os.getenv("DOCKER_ENV"):
    # ÎèÑÏª§ ÌôòÍ≤ΩÏóêÏÑúÎäî postgres Ìò∏Ïä§Ìä∏Î™Ö ÏÇ¨Ïö©
    DATABASE_URL = os.getenv(
        "DATABASE_URL", "postgresql+asyncpg://chartbeacon:chartbeacon123@postgres:5432/chartbeacon"
    )
else:
    # Î°úÏª¨ Í∞úÎ∞úÌôòÍ≤ΩÏóêÏÑúÎäî localhost ÏÇ¨Ïö©
    DATABASE_URL = os.getenv(
        "DATABASE_URL", "postgresql+asyncpg://chartbeacon:chartbeacon123@localhost:5432/chartbeacon"
    )
# asyncpg ÏßÅÏ†ë Ïó∞Í≤∞ÏùÑ ÏúÑÌï¥ prefix Ï†úÍ±∞
if DATABASE_URL.startswith("postgresql+asyncpg://"):
    DATABASE_URL = DATABASE_URL.replace("postgresql+asyncpg://", "postgresql://")


async def fill_historical_data(ticker: str, timeframes: List[str] = None, period: str = "max"):
    """
    ÌäπÏ†ï Ï¢ÖÎ™©Ïùò Í≥ºÍ±∞ Îç∞Ïù¥ÌÑ∞Î•º Ï±ÑÏö∞Îäî Î©îÏù∏ Ìï®Ïàò

    Args:
        ticker: Ï¢ÖÎ™© ÏΩîÎìú
        timeframes: ÌÉÄÏûÑÌîÑÎ†àÏûÑ Î¶¨Ïä§Ìä∏ ['5m', '1h', '1d', '5d', '1mo', '3mo']
        period: Îç∞Ïù¥ÌÑ∞ Í∏∞Í∞Ñ (max = ÏµúÎåÄÌïú Í∏¥ Í∏∞Í∞ÑÏúºÎ°ú MA200 Îì± Ï∂©Î∂ÑÌïú Îç∞Ïù¥ÌÑ∞ ÌôïÎ≥¥)
    """
    if timeframes is None:
        timeframes = ["5m", "1h", "1d", "5d", "1mo", "3mo"]

    logger.info(f"üöÄ Starting data fill for {ticker}, timeframes: {timeframes}, period: {period}")

    try:
        # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞
        conn = await asyncpg.connect(DATABASE_URL)

        # Ïã¨Î≥º ID Ï°∞Ìöå
        symbol_id = await get_symbol_id(conn, ticker)
        if not symbol_id:
            logger.error(f"Symbol {ticker} not found in database")
            return

        for timeframe in timeframes:
            logger.info(f"Processing {ticker} - {timeframe}")

            # 1. Yahoo FinanceÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Í∞ÄÏ†∏Ïò§Í∏∞
            df = fetch_yahoo_data(ticker, timeframe, period)
            if df is None or df.empty:
                logger.warning(f"No data found for {ticker} - {timeframe}")
                continue

            # 2. Ï∫îÎì§ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
            await save_candle_data(conn, symbol_id, timeframe, df)

            # 3. ÏßÄÌëú Í≥ÑÏÇ∞ Î∞è Ï†ÄÏû•
            await calculate_and_save_indicators(conn, symbol_id, timeframe, df)

            # 4. Ïù¥ÎèôÌèâÍ∑† Í≥ÑÏÇ∞ Î∞è Ï†ÄÏû•
            await calculate_and_save_moving_averages(conn, symbol_id, timeframe, df)

            # 5. ÏöîÏïΩ Í≥ÑÏÇ∞ Î∞è Ï†ÄÏû•
            await calculate_and_save_summary(conn, symbol_id, timeframe, df)

            logger.info(f"Completed processing {ticker} - {timeframe}")

        await conn.close()
        logger.info(f"‚úÖ Data fill completed for {ticker}")

    except Exception as e:
        logger.error(f"‚ùå Error filling data for {ticker}: {str(e)}")
        raise


def fetch_yahoo_data(ticker: str, timeframe: str, period: str = "max") -> Optional[pd.DataFrame]:
    """
    Yahoo FinanceÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Í∞ÄÏ†∏Ïò§Í∏∞
    """
    try:
        # ÌÉÄÏûÑÌîÑÎ†àÏûÑÎ≥Ñ interval Îß§Ìïë
        interval_map = {
            "5m": "5m",
            "1h": "1h",
            "1d": "1d",
            "5d": "5d",
            "1mo": "1mo",
            "3mo": "3mo",
        }

        interval = interval_map.get(timeframe)
        if not interval:
            logger.error(f"Unsupported timeframe: {timeframe}")
            return None

        # Îã®Í∏∞ Îç∞Ïù¥ÌÑ∞Îäî Í∏∞Í∞Ñ Ï†úÌïúÏù¥ ÏûàÏùå, Ïû•Í∏∞ Îç∞Ïù¥ÌÑ∞Îäî Ï∂©Î∂ÑÌïú Í∏∞Í∞Ñ ÌôïÎ≥¥
        if timeframe == "5m":
            period = "60d"  # 5Î∂Ñ: ÏµúÎåÄ 60Ïùº
        elif timeframe == "1h":
            period = "730d"  # 1ÏãúÍ∞Ñ: ÏµúÎåÄ 730Ïùº
        elif timeframe in ["5d", "1mo", "3mo"]:
            # Ïû•Í∏∞ Îç∞Ïù¥ÌÑ∞Îäî Îçî Í∏¥ Í∏∞Í∞Ñ ÏÇ¨Ïö©ÌïòÏó¨ MA200ÍπåÏßÄ Ï∂©Î∂ÑÌûà ÌôïÎ≥¥
            if timeframe == "5d":
                period = "10y"  # 5ÏùºÎ¥â: 10ÎÖÑ (MA200 ÌôïÎ≥¥)
            elif timeframe == "1mo":
                period = "max"  # 1ÏõîÎ¥â: ÏµúÎåÄ Í∏∞Í∞Ñ (MA200 ÌôïÎ≥¥)
            elif timeframe == "3mo":
                period = "max"  # 3ÏõîÎ¥â: ÏµúÎåÄ Í∏∞Í∞Ñ (MA50, MA200 ÌôïÎ≥¥)
        elif timeframe == "1d":
            # 1ÏùºÎ¥âÎèÑ Ï∂©Î∂ÑÌïú Í∏∞Í∞Ñ ÌôïÎ≥¥
            if period == "max":
                period = "max"
            else:
                period = "5y"  # Í∏∞Î≥∏Ï†ÅÏúºÎ°ú 5ÎÖÑ

        logger.info(f"Fetching {ticker} data: interval={interval}, period={period}")

        stock = yf.Ticker(ticker)
        df = stock.history(period=period, interval=interval)

        if df.empty:
            logger.warning(f"No data returned for {ticker}")
            return None

        # ÌÉÄÏûÑÏ°¥ Ï≤òÎ¶¨: Î™®Îì† Îç∞Ïù¥ÌÑ∞Î•º UTCÎ°ú ÌÜµÏùº
        logger.info(f"Before timezone processing - Index timezone: {df.index.tz}")
        logger.info(f"Index sample: {df.index[:3].tolist() if len(df.index) > 0 else 'Empty'}")

        try:
            # timezone-aware Ïó¨Î∂ÄÎ•º ÌôïÏã§ÌïòÍ≤å Ï≤¥ÌÅ¨
            has_tz = hasattr(df.index, "tz") and df.index.tz is not None
            logger.info(f"Has timezone: {has_tz}")

            if has_tz:
                # Ïù¥ÎØ∏ timezone Ï†ïÎ≥¥Í∞Ä ÏûàÏúºÎ©¥ UTCÎ°ú Î≥ÄÌôòÎßå
                logger.info(f"Converting from {df.index.tz} to UTC")
                df.index = df.index.tz_convert("UTC")
            else:
                # timezone Ï†ïÎ≥¥Í∞Ä ÏóÜÎäî Í≤ΩÏö∞
                if ticker.endswith(".KS"):
                    # ÌïúÍµ≠ Ï£ºÏãùÏùÄ KSTÎ°ú Í∞ÄÏ†ïÌïòÍ≥† UTCÎ°ú Î≥ÄÌôò
                    logger.info("Localizing to Asia/Seoul then converting to UTC")
                    df.index = df.index.tz_localize("Asia/Seoul").tz_convert("UTC")
                else:
                    # Îã§Î•∏ Ï£ºÏãùÎì§ÏùÄ UTCÎ°ú Í∞ÄÏ†ï
                    logger.info("Localizing to UTC")
                    df.index = df.index.tz_localize("UTC")

            logger.info(f"After timezone processing - Index timezone: {df.index.tz}")

        except Exception as tz_error:
            logger.error(f"Timezone conversion error for {ticker}: {tz_error}")
            logger.error(f"Error type: {type(tz_error)}")
            import traceback

            logger.error(f"Traceback: {traceback.format_exc()}")

            # ÏóêÎü¨ Î∞úÏÉù Ïãú Îã§Î•∏ Î∞©Î≤ï ÏãúÎèÑ
            try:
                # Î™®Îì† timezone Ï†ïÎ≥¥ Ï†úÍ±∞ÌïòÍ≥† UTCÎ°ú Îã§Ïãú ÏÑ§Ï†ï
                if hasattr(df.index, "tz_localize"):
                    df.index = df.index.tz_localize(None).tz_localize("UTC")
                    logger.info("Fallback: Removed timezone and re-localized to UTC")
                else:
                    df.index = pd.to_datetime(df.index, utc=True)
                    logger.info("Fallback: Converted to UTC using pd.to_datetime")
            except Exception as fallback_error:
                logger.error(f"Fallback also failed: {fallback_error}")
                # ÏµúÌõÑÏùò ÏàòÎã®: timezone Ï†ïÎ≥¥ ÏóÜÏù¥ ÏßÑÌñâ
                logger.warning("Proceeding without timezone conversion")
        df.columns = df.columns.str.lower()
        df = df.reset_index()

        # ÎîîÎ≤ÑÍπÖ: Ïã§Ï†ú Ïª¨ÎüºÎ™Ö ÌôïÏù∏
        logger.info(f"DataFrame columns after reset_index: {list(df.columns)}")
        logger.info(f"DataFrame index name: {df.index.name}")

        # Ïù∏Îç±Ïä§ Ïª¨ÎüºÎ™ÖÏù¥ Îã§Î•º Ïàò ÏûàÏúºÎØÄÎ°ú ÌôïÏù∏ ÌõÑ Î≥ÄÍ≤Ω
        if "date" in df.columns:
            df.rename(columns={"date": "ts"}, inplace=True)
        elif "datetime" in df.columns:
            df.rename(columns={"datetime": "ts"}, inplace=True)
        elif "Datetime" in df.columns:
            df.rename(columns={"Datetime": "ts"}, inplace=True)
        else:
            # Ï≤´ Î≤àÏß∏ Ïª¨ÎüºÏù¥ ÏãúÍ∞Ñ Ïª¨ÎüºÏùº Í∞ÄÎä•ÏÑ±
            first_col = df.columns[0]
            logger.warning(
                f"Expected 'date' or 'datetime' column not found. Using first column: {first_col}"
            )
            df.rename(columns={first_col: "ts"}, inplace=True)

        # ÌïúÍµ≠ Ï£ºÏãùÏùò Í≤ΩÏö∞ Ï†ïÍ∑úÏû• ÏãúÍ∞ÑÎßå ÌïÑÌÑ∞ÎßÅ
        if ticker.endswith(".KS") and timeframe in ["5m", "1h"]:
            logger.info(f"Filtering regular trading hours for {ticker}")

            # ts Ïª¨ÎüºÏù¥ Ïù¥ÎØ∏ UTC ÏãúÍ∞ÑÏù¥ÎØÄÎ°ú KSTÎ°ú Î≥ÄÌôòÌï¥ÏÑú ÌïÑÌÑ∞ÎßÅ
            ts_series = pd.to_datetime(df["ts"])
            logger.info(f"ts_series timezone: {ts_series.dt.tz}")

            # timezone Ï†ïÎ≥¥Í∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏ÌïòÍ≥† Ï≤òÎ¶¨
            if ts_series.dt.tz is not None:
                # Ïù¥ÎØ∏ timezoneÏù¥ ÏûàÏúºÎ©¥ KSTÎ°ú Î≥ÄÌôò
                df_ts_kst = ts_series.dt.tz_convert("Asia/Seoul")
            else:
                # timezoneÏù¥ ÏóÜÏúºÎ©¥ UTCÎ°ú Í∞ÄÏ†ïÌïòÍ≥† KSTÎ°ú Î≥ÄÌôò
                df_ts_kst = ts_series.dt.tz_localize("UTC").dt.tz_convert("Asia/Seoul")

            df["hour"] = df_ts_kst.dt.hour
            df["minute"] = df_ts_kst.dt.minute

            # Ï†ïÍ∑úÏû• ÏãúÍ∞Ñ: 09:00 ~ 15:30 (15:30 Ìè¨Ìï®)
            regular_hours = (
                ((df["hour"] == 9) & (df["minute"] >= 0))
                | ((df["hour"] >= 10) & (df["hour"] <= 14))
                | ((df["hour"] == 15) & (df["minute"] <= 30))
            )

            original_count = len(df)
            df = df[regular_hours].drop(["hour", "minute"], axis=1)
            filtered_count = len(df)

            logger.info(f"Regular hours filtering: {original_count} -> {filtered_count} records")

            if filtered_count == 0:
                logger.warning("All data filtered out! This might indicate timezone issues.")

        logger.info(f"Final DataFrame columns: {list(df.columns)}")
        logger.info(f"Fetched {len(df)} records for {ticker} - {timeframe}")
        return df

    except Exception as e:
        logger.error(f"Error fetching data for {ticker}: {str(e)}")
        return None


async def get_symbol_id(conn: asyncpg.Connection, ticker: str) -> Optional[int]:
    """
    Ïã¨Î≥º ID Ï°∞Ìöå
    """
    result = await conn.fetchrow("SELECT id FROM symbols WHERE ticker = $1", ticker)
    return result["id"] if result else None


async def save_candle_data(
    conn: asyncpg.Connection, symbol_id: int, timeframe: str, df: pd.DataFrame
):
    """
    Ï∫îÎì§ Îç∞Ïù¥ÌÑ∞Î•º Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ï†ÄÏû• (UPSERT)
    """
    logger.info(f"Saving {len(df)} candle records for symbol_id={symbol_id}, timeframe={timeframe}")

    # Î∞∞Ïπò insertÎ•º ÏúÑÌïú Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
    records = []
    for _, row in df.iterrows():
        records.append(
            (
                symbol_id,
                timeframe,
                row["ts"],
                float(row["open"]),
                float(row["high"]),
                float(row["low"]),
                float(row["close"]),
                float(row["volume"]),
                datetime.now(timezone.utc),
            )
        )

    # UPSERT ÏøºÎ¶¨
    query = """
        INSERT INTO candles_raw (symbol_id, timeframe, ts, open, high, low, close, volume, ingested_at)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
        ON CONFLICT (symbol_id, timeframe, ts) 
        DO UPDATE SET 
            open = EXCLUDED.open,
            high = EXCLUDED.high,
            low = EXCLUDED.low,
            close = EXCLUDED.close,
            volume = EXCLUDED.volume,
            ingested_at = EXCLUDED.ingested_at
    """

    await conn.executemany(query, records)
    logger.info(f"Saved {len(records)} candle records")


async def calculate_and_save_indicators(
    conn: asyncpg.Connection, symbol_id: int, timeframe: str, df: pd.DataFrame
):
    """
    Í∏∞Ïà†Ï†Å ÏßÄÌëú Í≥ÑÏÇ∞ Î∞è Ï†ÄÏû•
    """
    logger.info(f"Calculating indicators for symbol_id={symbol_id}, timeframe={timeframe}")

    # Îç∞Ïù¥ÌÑ∞ Í∏∏Ïù¥ ÌôïÏù∏
    if len(df) < 30:
        logger.warning(f"Insufficient data for indicators calculation: {len(df)} rows")
        return

    # ÏßÄÌëú Í≥ÑÏÇ∞ (ÏïàÏ†ÑÌïú Ï≤òÎ¶¨)
    try:
        # RSI
        df["rsi14"] = ta.rsi(df["close"], length=14)

        # Stochastic
        stoch = ta.stoch(df["high"], df["low"], df["close"], k=9, d=6)
        if stoch is not None and not stoch.empty:
            df["stoch_k"] = stoch.get("STOCHk_9_6_3", pd.Series(index=df.index, dtype=float))
            df["stoch_d"] = stoch.get("STOCHd_9_6_3", pd.Series(index=df.index, dtype=float))
        else:
            df["stoch_k"] = pd.Series(index=df.index, dtype=float)
            df["stoch_d"] = pd.Series(index=df.index, dtype=float)

        # MACD
        macd = ta.macd(df["close"], fast=12, slow=26, signal=9)
        if macd is not None and not macd.empty:
            df["macd"] = macd.get("MACD_12_26_9", pd.Series(index=df.index, dtype=float))
            df["macd_signal"] = macd.get("MACDs_12_26_9", pd.Series(index=df.index, dtype=float))
        else:
            df["macd"] = pd.Series(index=df.index, dtype=float)
            df["macd_signal"] = pd.Series(index=df.index, dtype=float)

        # ADX
        adx = ta.adx(df["high"], df["low"], df["close"], length=14)
        if adx is not None and not adx.empty:
            df["adx14"] = adx.get("ADX_14", pd.Series(index=df.index, dtype=float))
        else:
            df["adx14"] = pd.Series(index=df.index, dtype=float)

        # CCI
        df["cci14"] = ta.cci(df["high"], df["low"], df["close"], length=14)
        if df["cci14"] is None:
            df["cci14"] = pd.Series(index=df.index, dtype=float)

        # ATR
        df["atr14"] = ta.atr(df["high"], df["low"], df["close"], length=14)
        if df["atr14"] is None:
            df["atr14"] = pd.Series(index=df.index, dtype=float)

        # Williams %R
        df["willr14"] = ta.willr(df["high"], df["low"], df["close"], length=14)
        if df["willr14"] is None:
            df["willr14"] = pd.Series(index=df.index, dtype=float)

        # Ultimate Oscillator
        df["ultosc"] = ta.uo(df["high"], df["low"], df["close"])
        if df["ultosc"] is None:
            df["ultosc"] = pd.Series(index=df.index, dtype=float)

        # ROC
        df["roc"] = ta.roc(df["close"], length=12)
        if df["roc"] is None:
            df["roc"] = pd.Series(index=df.index, dtype=float)

        # Bull/Bear Power
        ema13 = ta.ema(df["close"], length=13)
        if ema13 is not None:
            df["bull_bear"] = df["high"] - ema13
        else:
            df["bull_bear"] = pd.Series(index=df.index, dtype=float)

        # ÎÜíÏùÄ/ÎÇÆÏùÄ ÏßÄÌëú (High - Low)
        df["highlow14"] = df["high"].rolling(14).max() - df["low"].rolling(14).min()

    except Exception as e:
        logger.error(f"Error calculating indicators: {str(e)}")
        # Ïã§Ìå® Ïãú Îπà SeriesÎ°ú Ï±ÑÏö∞Í∏∞
        for col in [
            "rsi14",
            "stoch_k",
            "stoch_d",
            "macd",
            "macd_signal",
            "adx14",
            "cci14",
            "atr14",
            "willr14",
            "ultosc",
            "roc",
            "bull_bear",
            "highlow14",
        ]:
            if col not in df.columns:
                df[col] = pd.Series(index=df.index, dtype=float)

    # Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
    records = []
    for _, row in df.iterrows():
        if pd.isna(row["rsi14"]):  # Ï¥àÍ∏∞ Î™á Í∞ú ÌñâÏùÄ ÏßÄÌëúÍ∞Ä Í≥ÑÏÇ∞ÎêòÏßÄ ÏïäÏùå
            continue

        records.append(
            (
                symbol_id,
                timeframe,
                row["ts"],
                safe_float(row.get("rsi14")),
                safe_float(row.get("stoch_k")),
                safe_float(row.get("stoch_d")),
                safe_float(row.get("macd")),
                safe_float(row.get("macd_signal")),
                safe_float(row.get("adx14")),
                safe_float(row.get("cci14")),
                safe_float(row.get("atr14")),
                safe_float(row.get("willr14")),
                safe_float(row.get("highlow14")),
                safe_float(row.get("ultosc")),
                safe_float(row.get("roc")),
                safe_float(row.get("bull_bear")),
                datetime.now(timezone.utc),
            )
        )

    if records:
        query = """
            INSERT INTO indicators (
                symbol_id, timeframe, ts, rsi14, stoch_k, stoch_d, 
                macd, macd_signal, adx14, cci14, atr14, willr14, highlow14, 
                ultosc, roc, bull_bear, calc_at
            )
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17)
            ON CONFLICT (symbol_id, timeframe, ts) 
            DO UPDATE SET 
                rsi14 = EXCLUDED.rsi14,
                stoch_k = EXCLUDED.stoch_k,
                stoch_d = EXCLUDED.stoch_d,
                macd = EXCLUDED.macd,
                macd_signal = EXCLUDED.macd_signal,
                adx14 = EXCLUDED.adx14,
                cci14 = EXCLUDED.cci14,
                atr14 = EXCLUDED.atr14,
                willr14 = EXCLUDED.willr14,
                highlow14 = EXCLUDED.highlow14,
                ultosc = EXCLUDED.ultosc,
                roc = EXCLUDED.roc,
                bull_bear = EXCLUDED.bull_bear,
                calc_at = EXCLUDED.calc_at
        """

        await conn.executemany(query, records)
        logger.info(f"Saved {len(records)} indicator records")


async def calculate_and_save_moving_averages(
    conn: asyncpg.Connection, symbol_id: int, timeframe: str, df: pd.DataFrame
):
    """
    Ïù¥ÎèôÌèâÍ∑† Í≥ÑÏÇ∞ Î∞è Ï†ÄÏû•
    """
    logger.info(f"Calculating moving averages for symbol_id={symbol_id}, timeframe={timeframe}")

    # Ïù¥ÎèôÌèâÍ∑† Í≥ÑÏÇ∞
    df["ma5"] = ta.sma(df["close"], length=5)
    df["ema5"] = ta.ema(df["close"], length=5)
    df["ma10"] = ta.sma(df["close"], length=10)
    df["ema10"] = ta.ema(df["close"], length=10)
    df["ma20"] = ta.sma(df["close"], length=20)
    df["ema20"] = ta.ema(df["close"], length=20)
    df["ma50"] = ta.sma(df["close"], length=50)
    df["ma100"] = ta.sma(df["close"], length=100)
    df["ma200"] = ta.sma(df["close"], length=200)

    # Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
    records = []
    for _, row in df.iterrows():
        if pd.isna(row["ma5"]):  # Ï¥àÍ∏∞ Î™á Í∞ú ÌñâÏùÄ Ïù¥ÎèôÌèâÍ∑†Ïù¥ Í≥ÑÏÇ∞ÎêòÏßÄ ÏïäÏùå
            continue

        records.append(
            (
                symbol_id,
                timeframe,
                row["ts"],
                safe_float(row.get("ma5")),
                safe_float(row.get("ema5")),
                safe_float(row.get("ma10")),
                safe_float(row.get("ema10")),
                safe_float(row.get("ma20")),
                safe_float(row.get("ema20")),
                safe_float(row.get("ma50")),
                safe_float(row.get("ma100")),
                safe_float(row.get("ma200")),
                datetime.now(timezone.utc),
            )
        )

    if records:
        query = """
            INSERT INTO moving_avgs (
                symbol_id, timeframe, ts, ma5, ema5, ma10, ema10, 
                ma20, ema20, ma50, ma100, ma200, calc_at
            )
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
            ON CONFLICT (symbol_id, timeframe, ts) 
            DO UPDATE SET 
                ma5 = EXCLUDED.ma5,
                ema5 = EXCLUDED.ema5,
                ma10 = EXCLUDED.ma10,
                ema10 = EXCLUDED.ema10,
                ma20 = EXCLUDED.ma20,
                ema20 = EXCLUDED.ema20,
                ma50 = EXCLUDED.ma50,
                ma100 = EXCLUDED.ma100,
                ma200 = EXCLUDED.ma200,
                calc_at = EXCLUDED.calc_at
        """

        await conn.executemany(query, records)
        logger.info(f"Saved {len(records)} moving average records")


async def calculate_and_save_summary(
    conn: asyncpg.Connection, symbol_id: int, timeframe: str, df: pd.DataFrame
):
    """
    ÏöîÏïΩ Ï†êÏàò Í≥ÑÏÇ∞ Î∞è Ï†ÄÏû•
    """
    logger.info(f"Calculating summary for symbol_id={symbol_id}, timeframe={timeframe}")

    # ÏµúÏã† 200Í∞ú ÌñâÎßå Ï≤òÎ¶¨ (Ï∂©Î∂ÑÌïú Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÎäî Íµ¨Í∞Ñ)
    df_recent = df.tail(200).copy()

    records = []
    for _, row in df_recent.iterrows():
        # ÏßÄÌëú Í∏∞Î∞ò ÏãúÍ∑∏ÎÑê Í≥ÑÏÇ∞
        signals = calculate_signals(row)

        buy_cnt = sum(1 for s in signals if s == "BUY")
        sell_cnt = sum(1 for s in signals if s == "SELL")
        neutral_cnt = sum(1 for s in signals if s == "NEUTRAL")

        # ÏµúÏ¢Ö Î†àÎ≤® Í≤∞Ï†ï
        total_signals = buy_cnt + sell_cnt + neutral_cnt
        if total_signals == 0:
            level = "NEUTRAL"
        elif buy_cnt >= (total_signals * 2 // 3):
            level = "STRONG_BUY"
        elif buy_cnt > sell_cnt:
            level = "BUY"
        elif sell_cnt >= (total_signals * 2 // 3):
            level = "STRONG_SELL"
        elif sell_cnt > buy_cnt:
            level = "SELL"
        else:
            level = "NEUTRAL"

        records.append(
            (
                symbol_id,
                timeframe,
                row["ts"],
                buy_cnt,
                sell_cnt,
                neutral_cnt,
                level,
                datetime.now(timezone.utc),
            )
        )

    if records:
        query = """
            INSERT INTO summary (
                symbol_id, timeframe, ts, buy_cnt, sell_cnt, 
                neutral_cnt, level, scored_at
            )
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
            ON CONFLICT (symbol_id, timeframe, ts) 
            DO UPDATE SET 
                buy_cnt = EXCLUDED.buy_cnt,
                sell_cnt = EXCLUDED.sell_cnt,
                neutral_cnt = EXCLUDED.neutral_cnt,
                level = EXCLUDED.level,
                scored_at = EXCLUDED.scored_at
        """

        await conn.executemany(query, records)
        logger.info(f"Saved {len(records)} summary records")


def calculate_signals(row) -> List[str]:
    """
    Í∞úÎ≥Ñ ÌñâÏùò ÏßÄÌëú Í∞íÏùÑ Í∏∞Î∞òÏúºÎ°ú ÏãúÍ∑∏ÎÑê Í≥ÑÏÇ∞
    """
    signals = []

    # RSI
    if pd.notna(row.get("rsi14")):
        if row["rsi14"] > 70:
            signals.append("SELL")
        elif row["rsi14"] < 30:
            signals.append("BUY")
        else:
            signals.append("NEUTRAL")

    # Stochastic
    if pd.notna(row.get("stoch_k")):
        if row["stoch_k"] > 80:
            signals.append("SELL")
        elif row["stoch_k"] < 20:
            signals.append("BUY")
        else:
            signals.append("NEUTRAL")

    # MACD
    if pd.notna(row.get("macd")) and pd.notna(row.get("macd_signal")):
        if row["macd"] > row["macd_signal"]:
            signals.append("BUY")
        elif row["macd"] < row["macd_signal"]:
            signals.append("SELL")
        else:
            signals.append("NEUTRAL")

    # CCI
    if pd.notna(row.get("cci14")):
        if row["cci14"] > 100:
            signals.append("BUY")
        elif row["cci14"] < -100:
            signals.append("SELL")
        else:
            signals.append("NEUTRAL")

    # Williams %R
    if pd.notna(row.get("willr14")):
        if row["willr14"] > -20:
            signals.append("SELL")
        elif row["willr14"] < -80:
            signals.append("BUY")
        else:
            signals.append("NEUTRAL")

    # ROC
    if pd.notna(row.get("roc")):
        if row["roc"] > 0:
            signals.append("BUY")
        elif row["roc"] < 0:
            signals.append("SELL")
        else:
            signals.append("NEUTRAL")

    # Ïù¥ÎèôÌèâÍ∑† ÏãúÍ∑∏ÎÑêÎì§
    ma_signals = []
    for ma_col in ["ma5", "ema5", "ma10", "ema10", "ma20", "ema20", "ma50", "ma100", "ma200"]:
        if pd.notna(row.get(ma_col)) and pd.notna(row.get("close")):
            if row["close"] > row[ma_col]:
                ma_signals.append("BUY")
            elif row["close"] < row[ma_col]:
                ma_signals.append("SELL")
            else:
                ma_signals.append("NEUTRAL")

    signals.extend(ma_signals)
    return signals


def safe_float(value) -> Optional[float]:
    """
    ÏïàÏ†ÑÌïú float Î≥ÄÌôò (NaN Ï≤òÎ¶¨)
    """
    if pd.isna(value):
        return None
    try:
        return float(value)
    except (ValueError, TypeError):
        return None
